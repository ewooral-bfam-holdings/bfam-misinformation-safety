# ğŸ§  Detecting and Preventing AI-Generated Misinformation in Low-Information Communities using Explainable Machine Learning

## ğŸ“˜ Overview
We are building simple and practical tools that detect and explain fake videos and AI-generated text that target people in low-information communities.  
Our goal is to give clear, fast, and understandable signals that a message or video may not be real and to explain **why** it was flagged.

This project is part of our AI safety work at **Ewooral & BFAM Holdings**, aimed at protecting rural and under-informed communities from misinformation, deepfakes, and manipulated media.

---

## âš–ï¸ Why This Matters
Many people in rural and low-information areas believe what they see or read online without questioning it.  
When false videos or AI-generated messages spread, they can harm reputations and create real-world damage.  

Our company experienced this firsthand when a **fake video** misrepresented our CEO. That incident motivated us to study **AI Safety** especially how to detect and reduce harmful AI-generated outputs.

This project links to AI safety by exploring **how models create deceptive or harmful content**, and **how we can detect and explain** such risks transparently.

---

## ğŸ” Main Research Question
> How can we detect AI-generated video and text in low-resource settings and provide short, human-readable explanations for detections?

---

## ğŸ¯ Objectives
- Develop a **deepfake video detection model** for low-resource setups.  
- Build a **text classifier** to detect AI-generated or manipulated messages.  
- Create **explainable AI tools** (XAI) that help non-experts understand model reasoning.  
- Test and validate on simulated datasets reflecting rural communication styles.  
- Produce a **short paper**, **GitHub repository**, and **demo blog post**.

---

## ğŸ“¦ Datasets and Data Plan

### Public Datasets
- **FaceForensics++** â€“ For video deepfake samples.  
- **DFDC (Deepfake Detection Challenge)** â€“ For large-scale detection benchmarking.  
- **Celeb-DF** â€“ For high-quality deepfake videos.  
- **Human-written & AI-generated text** â€“ Collected from news and LLM outputs.

### Local Data Creation
- Small sample of **community-style videos and messages** using local languages and names.  
- Synthetic social posts or WhatsApp-like messages, collected ethically with consent.  
- Focus on *Ghanaian rural communication formats* for authenticity.

---

## ğŸ”’ Ethics and Privacy
- Use only **public** or **consented** data.  
- Anonymize any human faces or identities in training data.  
- Maintain a `data_license.md` file listing sources and usage terms.  
- Follow AI ethics guidelines for misinformation research.

---

## ğŸ§° Methods and Tools

### Core Libraries
- `PyTorch` â€” Model training and fine-tuning  
- `Transformers` (Hugging Face) â€” For text-based detection  
- `OpenCV`, `FFmpeg` â€” Video and frame extraction  
- `MTCNN` or `dlib` â€” Face detection  
- `Librosa` â€” Audio feature extraction  
- `SHAP`, `LIME`, `Grad-CAM` â€” Model interpretability and explainability  

### Model Concepts

#### ğŸ Video Pipeline
1. Extract and preprocess frames using OpenCV  
2. Detect and crop faces per frame  
3. Train a lightweight CNN or fine-tuned pretrained backbone  
4. Combine temporal and (optionally) audio features  

#### âœ Text Pipeline
1. Train a transformer to detect synthetic text  
2. Analyze perplexity and unusual structure patterns  
3. Highlight tokens and words that triggered flags  

---

## ğŸ§© Explainability
- **Video:** Use Grad-CAM visual maps to show why a frame was flagged.  
- **Text:** Use SHAP or LIME to highlight suspicious word patterns or anomalies.  

---

## ğŸ“Š Evaluation Metrics
- Precision, Recall, F1 Score  
- ROC-AUC for classification quality  
- Robustness under compression, noise, and scaling  
- Human evaluation for explanation clarity  

---

## ğŸ—“ 12-Week Project Schedule

| Weeks | Task |
|-------|------|
| Week 0 | Setup repo, prepare datasets |
| Weeks 1â€“2 | Data loaders, baseline text/video models |
| Weeks 3â€“4 | Improve models, add temporal pooling |
| Week 5 | Add explainability features |
| Week 6 | Robustness testing and fine-tuning |
| Week 7 | Small local user study for explainability |
| Weeks 8â€“9 | Paper draft and documentation |
| Week 10 | Code cleanup and reproducibility checks |
| Week 11 | Final experiments and reporting |
| Week 12 | Deliverables (paper, blog, demo, GitHub push) |

---

## ğŸ§¾ Deliverables
- Public GitHub repo with full source code  
- Dataset samples and data generation scripts  
- Model checkpoints and evaluation logs  
- A 2-page workshop paper draft  
- Blog post + demo notebook explaining findings  

---

## ğŸ“ Repository Structure
```bash
bfam-misinformation-safety/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ model_video.py
â”‚   â”œâ”€â”€ model_text.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â””â”€â”€ explain.py
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ LICENSE



âš™ï¸ Installation and Setup
1ï¸âƒ£ Clone Repository